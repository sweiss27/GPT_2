{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f290a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"transformers[torch]\" datasets evaluate sacrebleu nltk psutil matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0185aeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5671aab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "raw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0460918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23767 2461\n",
      " = Valkyria Chronicles III = \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_texts(ds):\n",
    "    texts = [t for t in ds[\"text\"] if t and not t.isspace()]\n",
    "    return texts\n",
    "\n",
    "train_texts = clean_texts(raw[\"train\"])\n",
    "val_texts   = clean_texts(raw[\"validation\"])\n",
    "\n",
    "print(len(train_texts), len(val_texts))\n",
    "print(train_texts[0][:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b57167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# GPT-2 has no pad token by default, so we reuse EOS for padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=block_size,\n",
    "        padding=\"max_length\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea6eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threaded tokenization: 20000 texts in 1.26s using 4 threads\n",
      "Threaded tokenization: 2000 texts in 0.11s using 4 threads\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math, time\n",
    "\n",
    "def chunk_list(lst, n_chunks):\n",
    "    chunk_size = math.ceil(len(lst) / n_chunks)\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def threaded_tokenize(texts, n_threads=4):\n",
    "    chunks = chunk_list(texts, n_threads)\n",
    "\n",
    "    t0 = time.time()\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as ex:\n",
    "        futures = [ex.submit(tokenize_batch, c) for c in chunks]\n",
    "        for f in futures:\n",
    "            results.append(f.result())\n",
    "    t1 = time.time()\n",
    "\n",
    "    # merge dicts-of-lists\n",
    "    merged = {k: sum([r[k] for r in results], []) for k in results[0].keys()}\n",
    "    print(f\"Threaded tokenization: {len(texts)} texts in {t1-t0:.2f}s using {n_threads} threads\")\n",
    "    return merged\n",
    "\n",
    "train_tokens = threaded_tokenize(train_texts[:20000], n_threads=4)  # subset for faster runs\n",
    "val_tokens   = threaded_tokenize(val_texts[:2000], n_threads=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a32b72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "train_ds = Dataset.from_dict(train_tokens)\n",
    "val_ds   = Dataset.from_dict(val_tokens)\n",
    "\n",
    "def to_torch(batch):\n",
    "    return {k: torch.tensor(v) for k, v in batch.items()}\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61dd08ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from math import exp\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_perplexity(model, loader, device, max_eval_batches=100):\n",
    "    \"\"\"\n",
    "    Perplexity evaluation capped to avoid long eval runs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= max_eval_batches:\n",
    "            break\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        losses.append(out.loss.item())\n",
    "    model.train()\n",
    "    avg_loss = sum(losses) / max(1, len(losses))\n",
    "    return exp(avg_loss)\n",
    "\n",
    "def train_one_run(run_name, use_amp=False, use_ckpt=False, epochs=1, lr=5e-5, max_steps=300, log_every=50):\n",
    "    \"\"\"\n",
    "    Trains GPT-2 for a limited number of steps so experiments finish quickly.\n",
    "    Returns: (model, results_dict)\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    if use_ckpt:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # AMP setup\n",
    "    amp_active = (use_amp and device.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\") if amp_active else None\n",
    "\n",
    "    # memory tracking\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    step_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        for _, batch in enumerate(train_loader):\n",
    "            step_count += 1\n",
    "            if step_count > max_steps:\n",
    "                break\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if scaler is not None:\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    out = model(**batch)\n",
    "                    loss = out.loss\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                out = model(**batch)\n",
    "                loss = out.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if step_count == 1 or step_count % log_every == 0:\n",
    "                print(f\"[{run_name}] epoch {epoch} step {step_count}/{max_steps} loss {loss.item():.4f}\")\n",
    "\n",
    "        if step_count > max_steps:\n",
    "            break\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    peak_mb = None\n",
    "    if device.type == \"cuda\":\n",
    "        peak_mb = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # eval perplexity (capped for speed)\n",
    "    ppl = evaluate_perplexity(model, val_loader, device, max_eval_batches=100)\n",
    "\n",
    "    results = {\n",
    "        \"run\": run_name,\n",
    "        \"use_amp_requested\": use_amp,\n",
    "        \"use_amp_active\": amp_active,\n",
    "        \"use_ckpt\": use_ckpt,\n",
    "        \"epochs\": epochs,\n",
    "        \"max_steps\": max_steps,\n",
    "        \"train_time_sec\": round(t1 - t0, 2),\n",
    "        \"peak_gpu_mem_mb\": round(peak_mb, 2) if peak_mb is not None else None,\n",
    "        \"val_perplexity\": round(ppl, 3),\n",
    "    }\n",
    "    return model, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b732cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def bleu_on_one_sample(model, tokenizer, dataset, device, prompt_tokens=30, cont_tokens=30):\n",
    "    model.eval()\n",
    "    ids = dataset[0][\"input_ids\"]\n",
    "    prompt_ids = ids[:prompt_tokens]\n",
    "    ref_ids = ids[prompt_tokens:prompt_tokens+cont_tokens]\n",
    "\n",
    "    prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "    ref_text = tokenizer.decode(ref_ids, skip_special_tokens=True)\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=cont_tokens,\n",
    "        do_sample=False\n",
    "    )\n",
    "    gen_text = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "    # candidate = only the continuation portion (roughly)\n",
    "    candidate = gen_text.split()\n",
    "    reference = [ref_text.split()]\n",
    "\n",
    "    score = bleu.compute(predictions=[candidate], references=[reference])\n",
    "    model.train()\n",
    "    return prompt_text, ref_text, gen_text, score[\"bleu\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8115cf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3621984d52b4bf590fb6135fdfd8691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline_fp32] epoch 0 step 1/300 loss 4.9786\n",
      "[baseline_fp32] epoch 0 step 50/300 loss 4.3684\n",
      "[baseline_fp32] epoch 0 step 100/300 loss 3.1455\n",
      "[baseline_fp32] epoch 0 step 150/300 loss 3.8088\n",
      "[baseline_fp32] epoch 0 step 200/300 loss 3.8336\n",
      "[baseline_fp32] epoch 0 step 250/300 loss 3.6804\n",
      "[baseline_fp32] epoch 0 step 300/300 loss 3.7823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6008be35c0644d28ee1a8b291419f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mixed_precision] epoch 0 step 1/300 loss 4.3242\n",
      "[mixed_precision] epoch 0 step 50/300 loss 3.8912\n",
      "[mixed_precision] epoch 0 step 100/300 loss 3.7188\n",
      "[mixed_precision] epoch 0 step 150/300 loss 4.1064\n",
      "[mixed_precision] epoch 0 step 200/300 loss 3.6311\n",
      "[mixed_precision] epoch 0 step 250/300 loss 3.3621\n",
      "[mixed_precision] epoch 0 step 300/300 loss 3.5245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a9ae4ee03346da871fbb88370eec1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[amp_plus_checkpointing] epoch 0 step 1/300 loss 4.4466\n",
      "[amp_plus_checkpointing] epoch 0 step 50/300 loss 3.8051\n",
      "[amp_plus_checkpointing] epoch 0 step 100/300 loss 4.0113\n",
      "[amp_plus_checkpointing] epoch 0 step 150/300 loss 4.1325\n",
      "[amp_plus_checkpointing] epoch 0 step 200/300 loss 3.3475\n",
      "[amp_plus_checkpointing] epoch 0 step 250/300 loss 3.8197\n",
      "[amp_plus_checkpointing] epoch 0 step 300/300 loss 3.2242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'run': 'baseline_fp32',\n",
       "  'use_amp_requested': False,\n",
       "  'use_amp_active': False,\n",
       "  'use_ckpt': False,\n",
       "  'epochs': 1,\n",
       "  'max_steps': 300,\n",
       "  'train_time_sec': 1215.12,\n",
       "  'peak_gpu_mem_mb': None,\n",
       "  'val_perplexity': 33.026},\n",
       " {'run': 'mixed_precision',\n",
       "  'use_amp_requested': True,\n",
       "  'use_amp_active': False,\n",
       "  'use_ckpt': False,\n",
       "  'epochs': 1,\n",
       "  'max_steps': 300,\n",
       "  'train_time_sec': 1453.52,\n",
       "  'peak_gpu_mem_mb': None,\n",
       "  'val_perplexity': 32.926},\n",
       " {'run': 'amp_plus_checkpointing',\n",
       "  'use_amp_requested': True,\n",
       "  'use_amp_active': False,\n",
       "  'use_ckpt': True,\n",
       "  'epochs': 1,\n",
       "  'max_steps': 300,\n",
       "  'train_time_sec': 1654.75,\n",
       "  'peak_gpu_mem_mb': None,\n",
       "  'val_perplexity': 32.425}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs = []\n",
    "\n",
    "m0, r0 = train_one_run(\"baseline_fp32\", use_amp=False, use_ckpt=False, epochs=1)\n",
    "runs.append(r0)\n",
    "\n",
    "m1, r1 = train_one_run(\"mixed_precision\", use_amp=True, use_ckpt=False, epochs=1)\n",
    "runs.append(r1)\n",
    "\n",
    "m2, r2 = train_one_run(\"amp_plus_checkpointing\", use_amp=True, use_ckpt=True, epochs=1)\n",
    "runs.append(r2)\n",
    "\n",
    "runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cea3e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def bleu_on_one_sample(model, tokenizer, dataset, device, prompt_tokens=30, cont_tokens=30, max_tries=200):\n",
    "    \"\"\"\n",
    "    Computes smoothed BLEU on a single sample, but safely:\n",
    "    - Skips samples where the reference continuation decodes to empty (padding/EOS only)\n",
    "    - Scores only the generated continuation (not prompt+continuation)\n",
    "    - Uses smooth=True to avoid BLEU collapsing to 0 too easily\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    for idx in range(min(max_tries, len(dataset))):\n",
    "        ids = dataset[idx][\"input_ids\"]\n",
    "\n",
    "        prompt_ids = ids[:prompt_tokens]\n",
    "        ref_ids = ids[prompt_tokens:prompt_tokens + cont_tokens]\n",
    "\n",
    "        prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True).strip()\n",
    "        ref_text = tokenizer.decode(ref_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        # Skip empty references (common with padding)\n",
    "        if not ref_text:\n",
    "            continue\n",
    "\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=cont_tokens,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "        # Continuation only\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "        gen_cont_ids = gen_ids[0][prompt_len:]\n",
    "        gen_cont_text = tokenizer.decode(gen_cont_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if not gen_cont_text:\n",
    "            continue\n",
    "\n",
    "        bleu_score = bleu.compute(\n",
    "            predictions=[gen_cont_text],\n",
    "            references=[[ref_text]],\n",
    "            smooth=True\n",
    "        )[\"bleu\"]\n",
    "\n",
    "        model.train()\n",
    "        return prompt_text, ref_text, gen_cont_text, bleu_score\n",
    "\n",
    "    model.train()\n",
    "    raise ValueError(\"No valid sample found. Increase max_tries or adjust prompt_tokens/cont_tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23c2a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea\n",
      "\n",
      "REFERENCE:\n",
      " and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm\n",
      "\n",
      "GENERATED (continuation only):\n",
      " , and the Gulf of Mexico . It is a common lobster in the United States , Canada , and Mexico . It is a common lobster in the United\n",
      "\n",
      "Smoothed BLEU: 0.10832996189306149\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "prompt, ref, gen, bleu_score = bleu_on_one_sample(m2, tokenizer, val_ds, device)\n",
    "\n",
    "print(\"PROMPT:\\n\", prompt)\n",
    "print(\"\\nREFERENCE:\\n\", ref)\n",
    "print(\"\\nGENERATED (continuation only):\\n\", gen)\n",
    "print(\"\\nSmoothed BLEU:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66952426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e583fe41a24f6ca4fe8253384f1e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: gpt2_finetuned_outputs\n"
     ]
    }
   ],
   "source": [
    "import json, os, time\n",
    "\n",
    "out_dir = \"gpt2_finetuned_outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "m2.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "with open(os.path.join(out_dir, \"run_metrics.json\"), \"w\") as f:\n",
    "    json.dump(runs, f, indent=2)\n",
    "\n",
    "print(\"Saved to:\", out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
